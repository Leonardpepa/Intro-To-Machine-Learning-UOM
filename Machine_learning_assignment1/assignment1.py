# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LFrAmX-0Y7kLAFTbIUK5fjgLGpj3PPda
"""

#necessary python packages
import numpy as np                 #helps with numerical handling
import matplotlib.pyplot as plt    #helps with plots
import scipy.optimize as opt       #curve fitting tools  
import math

# 1
#generate some random values
numberOfGeneratedValues = 150
inputValues = np.sort(np.random.uniform(-4.0,4.0, numberOfGeneratedValues))

# 2
def myCustFunc(x, l1, l2):
  return l1 * (1 / np.exp(x)) + l2 * np.sin(x)

# 3
l1 = 0.18
l2 = 0.78
outputValues = myCustFunc(inputValues, l1, l2)

# 4
plt.plot(inputValues, '.', label='Random Input Numbers -4 - 4')
plt.plot( outputValues, '.', label=f'myCustFunc λ1:{l1} - λ2:{l2}', color='orange')
plt.xlabel ('input')
plt.ylabel ('output')
plt.title ('Current values')
plt.legend(loc='best')
plt.show()

# 5
# noisy values
noisyOutputValues = outputValues +\
 np.random.laplace(0., 0.85, numberOfGeneratedValues)

# 6

#original data
bestValsLinModel, CoVarLinModel =\
 opt.curve_fit(myCustFunc,inputValues, outputValues)
print('linear model estimated parameters (on original data) are:',\
      '{:.2f}'.format(bestValsLinModel[0]),\
      '{:.2f}'.format(bestValsLinModel[1]))

# noisy data
bestValsLinModel, CoVarLinModel =\
 opt.curve_fit(myCustFunc,inputValues, noisyOutputValues)
print('linear model estimated parameters (on noisy data) are:',\
      '{:.2f}'.format(bestValsLinModel[0]),\
      '{:.2f}'.format(bestValsLinModel[1]))

# 7
def poly4thDegree(x, a, b, c, d, e):
  return a*(x**4) + b*(x**3) + c*(x**2) + d*x + e

# 8
bestValsPolModel, CoVarPolModel =\
 opt.curve_fit(poly4thDegree,inputValues,noisyOutputValues)
print(bestValsPolModel)

# 9

#calculate the outputs using the curve fiiting estimated parameters
linnearModelPredictedOutcomes =\
 myCustFunc(inputValues, bestValsLinModel[0], bestValsLinModel[1] )

polyModelPredictedOutcomes =\
 poly4thDegree(inputValues, bestValsPolModel[0],\
             bestValsPolModel[1],bestValsPolModel[2], bestValsPolModel[3], bestValsPolModel[4])
 
 # start plotting the results
plt.plot(inputValues,outputValues, '.', label='Actual')
plt.plot(inputValues,linnearModelPredictedOutcomes, '.', label='LinEst')
plt.xlabel ('input')
plt.ylabel ('output')
plt.title ('MyCustFunc')
plt.legend(loc='best')
plt.show()

plt.plot(inputValues,outputValues, '.', label='Actual')
plt.plot(inputValues,polyModelPredictedOutcomes, '.', label='Poly4d')
plt.xlabel ('input')
plt.ylabel ('output')
plt.title ('Poly values')
plt.legend(loc='best')
plt.show()

# 10
from sklearn.metrics import mean_squared_error, mean_absolute_error
# original-noizy
rmseOriginalNoizy = math.sqrt(mean_squared_error(outputValues, noisyOutputValues))
maeOriginalNoizy = mean_absolute_error(outputValues, noisyOutputValues)

print("The root-mean-square error (RMSE) is used to measure the differences between \
  values predicted by our model and the values observed\n")

print("The mean absolute error (MAE) is a measure of errors between paired observations expressing \
 the same phenomenon.\n")


print("Original-Noizy")
print("--------------------")
print(f"Root Mean squared error: {rmseOriginalNoizy}")
print(f"Mean absolute error: {maeOriginalNoizy}")
print("--------------------")

# noizy - MyCustFunc
rmseNoizyMyCustFunc = math.sqrt(mean_squared_error(noisyOutputValues, linnearModelPredictedOutcomes))
maeNoizyMyCustFunc = mean_absolute_error(noisyOutputValues, linnearModelPredictedOutcomes)

print("Noizy - MyCustFunc")
print("--------------------")
print(f"Root Mean squared error: {rmseNoizyMyCustFunc}")
print(f"Mean absolute error: {maeNoizyMyCustFunc}")
print("--------------------")

# noizy - MyCustFunc
rmseNoizyPoly = math.sqrt(mean_squared_error(noisyOutputValues, polyModelPredictedOutcomes))
maeNoizyPoly = mean_absolute_error(noisyOutputValues, polyModelPredictedOutcomes)

print("Noizy - Poly4thDegree")
print("--------------------")
print(f"Root Mean squared error: {rmseNoizyPoly}")
print(f"Mean absolute error: {maeNoizyPoly}")
print("--------------------")

# Part 2

inputValues = inputValues.reshape(-1,1)

trainDatPerc = 0.7
valDatPerc = trainDatPerc/10
testDatPec = 1 - trainDatPerc - valDatPerc

# create shufled indexes for the data 
indices = np.random.permutation(len(inputValues))
trainInd = indices[0:int(trainDatPerc*len(indices))]
valInd = indices[int(trainDatPerc*len(indices)+1):\
                 int((trainDatPerc+valDatPerc)*len(indices))]
testInd = indices[int((trainDatPerc+valDatPerc)*len(indices))+1:]

# kNN regressor
from sklearn.neighbors import KNeighborsRegressor

# Initialize a model with 5 neighbors
knnReg = KNeighborsRegressor(n_neighbors=5)

# Fit the model
knnReg.fit(inputValues[trainInd],noisyOutputValues[trainInd])

# Gradient boosting regressor
from sklearn.ensemble import GradientBoostingRegressor

GBreg = GradientBoostingRegressor(random_state=0)
# Fit the model
GBreg.fit(inputValues[trainInd],noisyOutputValues[trainInd])

# use an SVR
from sklearn.svm import SVR

# Initialize a model with rbf kernel
svmReg =SVR(kernel='rbf')

# Fit the model
svmReg.fit(inputValues[trainInd],noisyOutputValues[trainInd])

#plot training set original and predicted output values

#first calculate predicted outputs (for train set only)
predictedOutputsknn = knnReg.predict(inputValues[trainInd])
predictedOutputssvm = svmReg.predict(inputValues[trainInd])
predictedOutputsgbr = GBreg.predict(inputValues[trainInd])

#now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyOutputValues[trainInd],'.', label = 'actual')
plt.plot(predictedOutputsknn,'rx', label = 'knn')
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputsgbr,'y*', label = 'bgr')
plt.legend(loc='best')
plt.show()

##plot test set original and predicted output values

#first calculate predicted outputs (for train set only)
predictedOutputsknn = knnReg.predict(inputValues[testInd])
predictedOutputssvm = svmReg.predict(inputValues[testInd])
predictedOutputsgbr = GBreg.predict(inputValues[testInd])

#now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyOutputValues[testInd],'.', label = 'actual')
plt.plot(predictedOutputsknn,'rx', label = 'knn')
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputsgbr,'y*', label = 'gbr')
plt.legend(loc='best')
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error,max_error

# calculate the scores
mae_knn = mean_absolute_error(noisyOutputValues[testInd], predictedOutputsknn)
rmse_knn = np.sqrt(mean_squared_error(noisyOutputValues[testInd],\
                                      predictedOutputsknn))
max_knn = max_error(noisyOutputValues[testInd], predictedOutputsknn)


mae_svm = mean_absolute_error(noisyOutputValues[testInd], predictedOutputssvm)
rmse_svm = np.sqrt(mean_squared_error(noisyOutputValues[testInd],\
                                      predictedOutputssvm))
max_svm = max_error(noisyOutputValues[testInd], predictedOutputssvm)

mae_gbr = mean_absolute_error(noisyOutputValues[testInd], predictedOutputsgbr)
rmse_gbr = np.sqrt(mean_squared_error(noisyOutputValues[testInd],\
                                      predictedOutputsgbr))
max_gbr = max_error(noisyOutputValues[testInd], predictedOutputsgbr)



#print them on screen
print('the knn regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_knn),\
      'RMSE: {:.2f}'.format(rmse_knn),\
      'max error: {:.2f}'.format(max_knn))

print('the svm regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_svm),\
      'RMSE: {:.2f}'.format(rmse_svm),\
      'max error: {:.2f}'.format(max_svm))

print('the gbr regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_gbr),\
      'RMSE: {:.2f}'.format(rmse_gbr),\
      'max error: {:.2f}'.format(max_gbr))

print("normalize the input values using a build in scaler in [0,1]")
from sklearn.preprocessing import MinMaxScaler
scaler_inputs = MinMaxScaler()
inputValues = scaler_inputs.fit_transform(inputValues.reshape(-1,1))

trainDatPerc = 0.7
valDatPerc = trainDatPerc/10
testDatPec = 1 - trainDatPerc - valDatPerc

# create shufled indexes for the data 
indices = np.random.permutation(len(inputValues))
trainInd = indices[0:int(trainDatPerc*len(indices))]
valInd = indices[int(trainDatPerc*len(indices)+1):\
                 int((trainDatPerc+valDatPerc)*len(indices))]
testInd = indices[int((trainDatPerc+valDatPerc)*len(indices))+1:]

# kNN regressor
from sklearn.neighbors import KNeighborsRegressor

# Initialize a model with 5 neighbors
knnReg = KNeighborsRegressor(n_neighbors=5)

# Fit the model
knnReg.fit(inputValues[trainInd],noisyOutputValues[trainInd])

# Gradient boosting regressor
from sklearn.ensemble import GradientBoostingRegressor

GBreg = GradientBoostingRegressor(random_state=0)
# Fit the model
GBreg.fit(inputValues[trainInd],noisyOutputValues[trainInd])

# use an SVR
from sklearn.svm import SVR

# Initialize a model with rbf kernel
svmReg =SVR(kernel='rbf')

# Fit the model
svmReg.fit(inputValues[trainInd],noisyOutputValues[trainInd])

#plot training set original and predicted output values

#first calculate predicted outputs (for train set only)
predictedOutputsknn = knnReg.predict(inputValues[trainInd])
predictedOutputssvm = svmReg.predict(inputValues[trainInd])
predictedOutputsgbr = GBreg.predict(inputValues[trainInd])

#now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyOutputValues[trainInd],'.', label = 'actual')
plt.plot(predictedOutputsknn,'rx', label = 'knn')
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputsgbr,'y*', label = 'bgr')
plt.legend(loc='best')
plt.show()

##plot test set original and predicted output values

#first calculate predicted outputs (for train set only)
predictedOutputsknn = knnReg.predict(inputValues[testInd])
predictedOutputssvm = svmReg.predict(inputValues[testInd])
predictedOutputsgbr = GBreg.predict(inputValues[testInd])

#now the ploting
plt.figure(figsize=(12, 4)) # define size to better illustrate results
plt.plot(noisyOutputValues[testInd],'.', label = 'actual')
plt.plot(predictedOutputsknn,'rx', label = 'knn')
plt.plot(predictedOutputssvm,'g+', label = 'svr')
plt.plot(predictedOutputsgbr,'y*', label = 'gbr')
plt.legend(loc='best')
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error,max_error

# calculate the scores
mae_knn = mean_absolute_error(noisyOutputValues[testInd], predictedOutputsknn)
rmse_knn = np.sqrt(mean_squared_error(noisyOutputValues[testInd],\
                                      predictedOutputsknn))
max_knn = max_error(noisyOutputValues[testInd], predictedOutputsknn)


mae_svm = mean_absolute_error(noisyOutputValues[testInd], predictedOutputssvm)
rmse_svm = np.sqrt(mean_squared_error(noisyOutputValues[testInd],\
                                      predictedOutputssvm))
max_svm = max_error(noisyOutputValues[testInd], predictedOutputssvm)

mae_gbr = mean_absolute_error(noisyOutputValues[testInd], predictedOutputsgbr)
rmse_gbr = np.sqrt(mean_squared_error(noisyOutputValues[testInd],\
                                      predictedOutputsgbr))
max_gbr = max_error(noisyOutputValues[testInd], predictedOutputsgbr)



#print them on screen
print('the knn regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_knn),\
      'RMSE: {:.2f}'.format(rmse_knn),\
      'max error: {:.2f}'.format(max_knn))

print('the svm regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_svm),\
      'RMSE: {:.2f}'.format(rmse_svm),\
      'max error: {:.2f}'.format(max_svm))

print('the gbr regresor (test) scores are:',\
      'MAE: {:.2f}'.format(mae_gbr),\
      'RMSE: {:.2f}'.format(rmse_gbr),\
      'max error: {:.2f}'.format(max_gbr))